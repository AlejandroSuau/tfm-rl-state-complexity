\chapter{Estado del arte}

\section{Fundamentos del Aprendizaje por Refuerzo}

El \textit{Aprendizaje por Refuerzo} (Reinforcement Learning, RL) constituye una de las ramas más relevantes del aprendizaje automático orientadas a la toma de decisiones secuenciales bajo incertidumbre. En este paradigma, un agente interactúa con un entorno mediante un proceso de prueba y error, buscando maximizar una recompensa acumulada a largo plazo. Formalmente, este proceso se modela como un \textit{Proceso de Decisión de Markov} (MDP), definido por el conjunto $(S, A, P, R, \gamma)$, donde $S$ representa el espacio de estados, $A$ el conjunto de acciones posibles, $P(s'|s,a)$ la función de transición de estados, $R(s,a)$ la función de recompensa y $\gamma \in [0,1]$ el factor de descuento que pondera la importancia de las recompensas futuras \citep{SuttonBarto2018}.

El objetivo de un agente es encontrar una política óptima $\pi^*(a|s)$ que maximice el retorno esperado:

\begin{equation}
G_t = \mathbb{E}\left[\sum_{k=0}^{\infty} \gamma^k r_{t+k+1}\right],
\end{equation}

donde $r_{t+k+1}$ representa la recompensa obtenida $k$ pasos después del tiempo $t$.  
A diferencia del aprendizaje supervisado, el RL no dispone de un conjunto fijo de ejemplos etiquetados: el conocimiento se adquiere mediante la experiencia directa con el entorno.  

Los algoritmos de RL pueden clasificarse en dos grandes familias.  
Los \textbf{métodos basados en valor} estiman la utilidad esperada de los estados o pares estado–acción a través de funciones $V(s)$ o $Q(s,a)$, como en los algoritmos Q-Learning o SARSA.  
Por otro lado, los \textbf{métodos basados en política} parametrizan directamente la política $\pi_\theta(a|s)$ mediante un modelo (típicamente una red neuronal) y optimizan sus parámetros $\theta$ a partir del gradiente de la recompensa esperada.  
La combinación de ambos enfoques da lugar a los métodos \textbf{actor-crítico}, donde un componente (\textit{crítico}) estima el valor de los estados y otro (\textit{actor}) actualiza la política en función de dicha estimación.

En este marco, la \textbf{definición del estado} $s_t$ es un factor determinante.  
La información que el agente percibe condiciona su capacidad para generalizar, explorar y aprender políticas efectivas. Una representación excesivamente simple puede ocultar detalles relevantes del entorno, mientras que una observación demasiado compleja puede introducir ruido, ralentizar la convergencia y requerir redes de mayor capacidad.  
Por ello, la \textbf{complejidad del estado observado} constituye una variable crítica para la eficiencia del aprendizaje, y su análisis es el eje central del presente trabajo.

\vspace{0.5em}

\section{Avances en Deep Reinforcement Learning}

La integración del aprendizaje por refuerzo con redes neuronales profundas dio lugar al \textit{Aprendizaje por Refuerzo Profundo} (Deep Reinforcement Learning, DRL), que permite aproximar funciones de valor o políticas directamente a partir de observaciones de alta dimensión, como imágenes o secuencias temporales.  
Este paradigma ha sido fundamental para extender el RL a entornos complejos donde las representaciones manuales del estado resultan inviables \citep{Mnih2015}.

\subsection*{Deep Q-Network (DQN)}

El punto de inflexión en la historia del DRL se produjo con el algoritmo \textbf{Deep Q-Network (DQN)} desarrollado por Mnih et al. \citeyearpar{Mnih2015}, que combinó \textit{Q-Learning} con redes neuronales convolucionales.  
DQN aprende una función de acción-valor $Q(s,a;\theta)$ que estima la recompensa esperada al ejecutar una acción $a$ en un estado $s$, siguiendo una política $\pi$ derivada de dicha estimación (normalmente, una política $\epsilon$-greedy).  
El objetivo de la red es minimizar la diferencia entre la predicción actual y el valor objetivo definido por la ecuación de Bellman:

\begin{equation}
L(\theta) = \mathbb{E}\left[(r + \gamma \max_{a'} Q(s',a';\theta^-) - Q(s,a;\theta))^2\right],
\end{equation}

donde $\theta^-$ son los parámetros de una red objetivo (\textit{target network}) que se actualiza de forma retardada para estabilizar el aprendizaje.  
El uso de una memoria de experiencias (\textit{replay buffer}) permite muestrear de manera aleatoria transiciones $(s,a,r,s')$, reduciendo la correlación temporal entre los datos.  

Gracias a estos mecanismos, DQN consiguió por primera vez que un agente alcanzara rendimientos comparables a los humanos en múltiples videojuegos de Atari, aprendiendo directamente a partir de píxeles sin información previa sobre las reglas del entorno.

A partir de este modelo original surgieron numerosas variantes orientadas a mejorar la estabilidad y eficiencia, como \textit{Double DQN}, que corrige el sesgo optimista de la estimación del valor máximo, o \textit{Dueling DQN}, que separa la estimación del valor de estado y la ventaja de cada acción.  

\subsection*{Advantage Actor-Critic (A2C)}

A diferencia de DQN, los métodos basados exclusivamente en valor no se adaptan bien a entornos con espacios de acción continuos.  
Para superar esta limitación surgieron los \textbf{métodos actor-crítico}, que combinan una \textit{red actor}, encargada de representar la política $\pi_\theta(a|s)$, y una \textit{red crítico}, que estima la función de valor $V_\phi(s)$ y evalúa la calidad de las acciones seleccionadas.  
El algoritmo \textbf{Advantage Actor-Critic (A2C)}, versión síncrona del A3C (\textit{Asynchronous Advantage Actor-Critic}) \citep{Mnih2016}, ejecuta múltiples agentes en paralelo que comparten la misma política y sincronizan sus actualizaciones periódicamente.

El crítico proporciona una señal de error (\textit{advantage}) definida como:
\begin{equation}
A(s,a) = Q(s,a) - V(s),
\end{equation}
que mide cuánto mejor resulta ejecutar la acción $a$ frente al valor medio esperado del estado.  
El actor ajusta su política en la dirección del gradiente del rendimiento esperado:
\begin{equation}
\nabla_\theta J(\theta) = \mathbb{E}[A(s,a)\nabla_\theta \log \pi_\theta(a|s)].
\end{equation}
Este esquema reduce la varianza del gradiente respecto a los métodos puramente basados en política, logrando un equilibrio entre estabilidad y eficiencia de aprendizaje.

\subsection*{Proximal Policy Optimization (PPO)}

Posteriormente, Schulman et al. \citeyearpar{Schulman2017} introdujeron el algoritmo \textbf{Proximal Policy Optimization (PPO)}, que mejoró la estabilidad del entrenamiento mediante una actualización de política acotada.  
En lugar de aplicar grandes cambios en la política tras cada actualización, PPO optimiza una función de pérdida que penaliza desviaciones excesivas entre la política nueva $\pi_\theta$ y la antigua $\pi_{\theta_\text{old}}$:

\begin{equation}
L^{CLIP}(\theta) = \mathbb{E}\left[\min\left(r_t(\theta)A_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon)A_t\right)\right],
\end{equation}

donde $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_\text{old}}(a_t|s_t)}$ representa la proporción entre políticas.  
El operador \textit{clip} limita la magnitud del cambio a un rango controlado $(1 \pm \epsilon)$, asegurando una mejora progresiva sin sobreajustes bruscos.  
Este mecanismo, junto con el uso de minibatches y actualizaciones múltiples por lote, convierte a PPO en un algoritmo especialmente robusto y fácil de entrenar, siendo actualmente uno de los más utilizados en investigación y aplicaciones prácticas.

\subsection*{Síntesis comparativa}

En síntesis, DQN, A2C y PPO representan tres paradigmas complementarios del Aprendizaje por Refuerzo Profundo:
\begin{itemize}
    \item \textbf{DQN:} aprendizaje basado en valor, idóneo para entornos con acciones discretas; su estabilidad depende fuertemente de la correlación entre muestras y de la calidad de la observación.
    \item \textbf{A2C:} enfoque actor-crítico que combina estimación de valor y política, reduciendo la varianza del gradiente y favoreciendo el aprendizaje paralelo.
    \item \textbf{PPO:} optimización de políticas proximales que introduce una restricción explícita sobre el cambio de política, logrando robustez y generalización en entornos complejos.
\end{itemize}

Estas características los convierten en candidatos idóneos para analizar de forma comparativa cómo la \textbf{complejidad de la observación} afecta el rendimiento del aprendizaje, manteniendo constante la dinámica y la recompensa del entorno.


\vspace{0.5em}

\section{Síntesis y vacíos detectados}

La revisión de la literatura evidencia que, aunque el Aprendizaje por Refuerzo Profundo ha experimentado avances notables en la última década, gran parte de los esfuerzos se ha centrado en la \textbf{optimización de los algoritmos} —ya sea mediante arquitecturas, estrategias de exploración o funciones de pérdida—, mientras que el impacto de la representación del estado ha recibido menor atención.  
En la mayoría de los estudios, el espacio de observación se trata como una variable fija del entorno, sin evaluar de manera sistemática cómo su complejidad o riqueza informacional influye en la eficacia del aprendizaje.

En respuesta a esta limitación, han surgido líneas de investigación como el \textit{State Representation Learning} (SRL), que busca aprender representaciones latentes más compactas y relevantes para la toma de decisiones \citep{Lesort2018,Ha2018,Hafner2020}.  
Estos trabajos utilizan modelos generativos o redes autoencoder para capturar información útil en espacios de menor dimensión, lo que facilita la generalización y reduce el coste computacional.  
No obstante, la mayoría de enfoques SRL se orientan a la \textbf{construcción automática de representaciones} y no al estudio explícito del efecto de distintos niveles de complejidad observacional sobre el proceso de aprendizaje.

De forma complementaria, los estudios sobre entornos parcialmente observables \citep{Hausknecht2015} han puesto de manifiesto cómo la falta de información puede limitar la convergencia y provocar políticas subóptimas.  
Sin embargo, apenas existen análisis controlados sobre el efecto contrario: cómo el exceso de información o la redundancia en el estado pueden degradar el rendimiento o ralentizar la convergencia.  

Los entornos más utilizados como \textit{benchmarks}, tales como Atari o MuJoCo, ofrecen espacios de observación fijos, lo que impide aislar experimentalmente esta variable.  
En años recientes, entornos modulares como \textit{MiniGrid} \citep{ChevalierBoisvert2018} o \textit{ProcGen} \citep{Cobbe2020} han permitido cierta variabilidad estructural, aunque su propósito principal ha sido evaluar la capacidad de generalización entre tareas, no la complejidad del estado como tal.

En este contexto, se identifica un vacío experimental relevante: la ausencia de estudios que analicen de forma controlada la relación entre la \textbf{complejidad del espacio de observación} y el rendimiento de distintos algoritmos de RL manteniendo constantes las dinámicas, recompensas y reglas del entorno.  

El presente trabajo busca contribuir a llenar ese vacío mediante el desarrollo de un entorno propio, \textit{SimplePacmanEnv}, inspirado en el clásico juego Pac-Man.  
Dicho entorno permite parametrizar el nivel de información observable, desde representaciones mínimas (posición del agente y del fantasma) hasta configuraciones enriquecidas con comodines, distribución de monedas o vistas tipo imagen.  
Esta configuración experimental posibilita evaluar empíricamente cómo la cantidad y naturaleza de la información percibida influyen en la estabilidad del entrenamiento, la velocidad de convergencia y la calidad final de la política aprendida.

En conjunto, este análisis pretende aportar una visión empírica y reproducible sobre el equilibrio entre \textbf{simplicidad y expresividad} en el diseño de observaciones, contribuyendo así a una comprensión más profunda del papel de la percepción en el rendimiento de los agentes de aprendizaje por refuerzo.
